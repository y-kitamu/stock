{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"前処理済みのデータを読み込むクラス\n",
    "    Args:\n",
    "        data (pl.DataFrame): 前処理済みのデータ (特徴量計算済み、欠損値処理済み)\n",
    "        window_size (int): 何日分のデータを取得するか\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pl.DataFrame, window_size: int):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.window_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> np.ndarray:\n",
    "        if 0 <= idx < len(self.data):\n",
    "            return self.data.slice(idx, self.window_size).to_numpy().flatten()\n",
    "        else:\n",
    "            raise IndexError\n",
    "        \n",
    "    def get_price(self, idx: int) -> float:\n",
    "        return self.data[\"close\"][idx + self.window_size - 1]\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self) -> gym.spaces.Space:\n",
    "        return gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(int(self.window_size * len(self.data.columns)),),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"前処理済み（特徴量計算済み）のpl.DataFrameを入力として、\n",
    "    step毎に状態を変化させ、報酬を返す環境クラス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.dataloader = env_config[\"dataloader\"]\n",
    "        self.current_index = 1\n",
    "        self.total_reward = 0\n",
    "        self.history = []\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = self.dataloader.observation_space\n",
    "    \n",
    "    def _calc_reward(self, action) -> float:\n",
    "        cur = self.dataloader.get_price(self.current_index)\n",
    "        pre = self.dataloader.get_price(self.current_index - 1)\n",
    "        action = 1 if action == 1 else -1\n",
    "        reward = (cur - pre) * action\n",
    "        return reward\n",
    "\n",
    "    def step(self, action)-> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n",
    "        if self.current_index >= len(self.dataloader):\n",
    "            raise RuntimeError(\"Episode is done. Please reset the environment.\")\n",
    "        reward = self._calc_reward(action)\n",
    "        obs = self.dataloader[self.current_index]\n",
    "        self.current_index += 1\n",
    "        is_terminated = self.current_index >= len(self.dataloader)\n",
    "        is_truncated = False\n",
    "        info = {\n",
    "            \"index\": self.current_index - 1,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"total_reward\": self.total_reward,\n",
    "            \"is_terminated\": is_terminated,\n",
    "            \"is_truncated\": is_truncated,\n",
    "        }\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        self.history.append(info)\n",
    "\n",
    "        return obs, reward, is_terminated, is_truncated, info\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[np.ndarray, dict[str, Any]]:\n",
    "        self.current_index = 1\n",
    "        self.total_reward = 0\n",
    "        self.history = []\n",
    "        return self.dataloader[0], {}\n",
    "\n",
    "    def render(self):\n",
    "        if len(self.history) == 0:\n",
    "            return\n",
    "        \n",
    "        df = pl.from_dicts(self.history)\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"index\").map_elements(lambda s: self.dataloader.get_price(s), return_dtype=pl.Float64).alias(\"price\"),\n",
    "        )\n",
    "        \n",
    "        sellbuy_df = df.filter((pl.col(\"action\") != pl.col(\"action\").shift(1)).fill_null(True))\n",
    "        sell_df = sellbuy_df.filter(pl.col(\"action\") == 0)\n",
    "        buy_df = sellbuy_df.filter(pl.col(\"action\") == 1)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        axes.plot(df[\"index\"], df[\"price\"], label=\"price\")\n",
    "        axes.scatter(sell_df[\"index\"], sell_df[\"price\"], marker=\"^\", color=\"red\", label=\"sell\")\n",
    "        axes.scatter(buy_df[\"index\"], buy_df[\"price\"], marker=\"v\", color=\"green\", label=\"buy\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artifitial_data(window_size, num_data_points) -> pl.DataFrame:\n",
    "    \"\"\"sine waveデータを生成する\"\"\"\n",
    "    t = np.linspace(0, np.pi * 6, int(num_data_points))\n",
    "    sine_wave = np.sin(t) * 50 + 100\n",
    "    sine_wave_with_pad = np.concatenate([np.zeros(int(window_size) - 1), sine_wave])\n",
    "    data = pl.DataFrame({\n",
    "        \"close\": sine_wave_with_pad,\n",
    "    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "num_data_points = 1000\n",
    "data = create_artifitial_data(window_size, num_data_points)\n",
    "dataloader = DataLoader(data, 25)\n",
    "assert len(dataloader) == num_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,    \n",
    "    ).learners(\n",
    "        num_learners=0,\n",
    "        num_gpus_per_learner=0,\n",
    "    ).training(\n",
    "        gamma=0,\n",
    "        lr_schedule=[\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        lr=8e-6,\n",
    "        model={\"uses_new_env_runners\": True},\n",
    "        lambda_=0.72,\n",
    "        vf_loss_coeff=0.5,\n",
    "        entropy_coeff=0.01,\n",
    "    ).environment(\n",
    "        clip_rewards=True,\n",
    "        env=TradingEnv,\n",
    "        env_config={\"dataloader\": dataloader},\n",
    "    )\n",
    ")\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray.rllib.algorithms import ppo\n",
    "#algo = ppo.PPO(env=TradingEnv, config={\"env_config\": {\"dataloader\": dataloader}})\n",
    "\n",
    "while True:\n",
    "    result = algo.train()\n",
    "    if result[\"env_runners\"][\"episode_return_mean\"] > 500:\n",
    "        break\n",
    "    print(\"Episode reward mean: \", result[\"env_runners\"][\"episode_return_mean\"])\n",
    "\n",
    "checkpoint_dir = algo.save_to_path()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = Algorithm.from_checkpoint(checkpoint_dir)\n",
    "rl_module = algo.get_module()\n",
    "print(rl_module.input_specs_inference())\n",
    "\n",
    "env = TradingEnv({\"dataloader\": dataloader})\n",
    "obs, info = env.reset()\n",
    "while True:\n",
    "    input = torch.from_numpy(np.array([obs]))\n",
    "    action_logits = rl_module.forward_inference({\"obs\": input})[\"action_dist_inputs\"]\n",
    "    action = torch.argmax(action_logits[0]).numpy()\n",
    "    obs, reward, is_terminated, is_truncated, _ = env.step(action)\n",
    "    if is_terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
