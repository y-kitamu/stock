{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機械学習導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import stock\n",
    "\n",
    "train_data_dir = stock.PROJECT_ROOT / \"data\" / \"train\"\n",
    "output_file_path = train_data_dir / \"{}.npz\".format(datetime.date.today().strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps、純利益から時価総額を計算する\n",
    "def calc_estimated_capitalization(code, current_date=datetime.date.today()):\n",
    "    fdf = stock.kabutan.read_financial_csv(code).filter(\n",
    "        (pl.col(\"duration\") == 3) & (pl.col(\"eps\").abs() > 1e-5)\n",
    "    ).sort(pl.col(\"annoounce_date\"))\n",
    "    df = stock.kabutan.read_data_csv(code, end_date=current_date).sort(pl.col(\"date\"))\n",
    "\n",
    "    if len(fdf) == 0:\n",
    "        return -1\n",
    "    num_stock = fdf[\"net_income\"][-1] * 1000000 / fdf[\"eps\"][-1]\n",
    "    est_capit = num_stock * df[\"close\"][-1]\n",
    "    return est_capit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずは学習データ準備\n",
    "target_data_dict = {}\n",
    "stacked = []\n",
    "codes = stock.kabutan.get_code_list()\n",
    "max_hold_days = 10\n",
    "\n",
    "for code in codes:\n",
    "    capt = calc_estimated_capitalization(code)\n",
    "    if capt > 100000000000: # 時価総額1000億円以上の場合はスキップ\n",
    "        continue\n",
    "    \n",
    "    df = stock.trend_template.calc_for_watch_list(code)\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"close\").rolling_max(window_size=max_hold_days).shift(-max_hold_days) / pl.col(\"open\").shift(-1)).alias(\"growing_rate\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        ((pl.col(\"growing_rate\") - 1.0)* 100).log().alias(\"log_growing_rate\")\n",
    "    )\n",
    "    target_data_dict[code] = df\n",
    "    stacked.append(df.filter(pl.col(\"watch_list\")).with_columns(pl.lit(code).alias(\"code\")))\n",
    "\n",
    "stacked_df = pl.concat(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとvalidの分割日を決定する\n",
    "dates = stacked_df.sort(pl.col(\"date\"))[\"date\"]\n",
    "# この日付までをtrain、これより先をvalidationとする\n",
    "split_date = dates[int(len(dates) * 0.8)]\n",
    "\n",
    "train_df = stacked_df.filter(pl.col(\"date\") <= split_date)\n",
    "valid_df = stacked_df.filter(pl.col(\"date\") > split_date)\n",
    "print(\"Split date = {}, num train = {}, num_valid = {}\".format(split_date, len(train_df), len(valid_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データは直前x日分のcloseとvolumeにする\n",
    "data_days = 30\n",
    "\n",
    "def get_data_list(df):\n",
    "    input_data_list = []\n",
    "    true_data_list = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        code = df[\"code\"][i]\n",
    "        date = df[\"date\"][i]\n",
    "        fdf = target_data_dict[code].filter(pl.col(\"date\") <= date)\n",
    "        if len(fdf) < data_days:\n",
    "            continue\n",
    "\n",
    "        open = fdf[\"open\"].to_numpy()[-data_days:]\n",
    "        high = fdf[\"high\"].to_numpy()[-data_days:]\n",
    "        low = fdf[\"low\"].to_numpy()[-data_days:]\n",
    "        close = fdf[\"close\"].to_numpy()[-data_days:]\n",
    "        volume = fdf[\"volume\"].to_numpy()[-data_days:]\n",
    "        #return date, close, volume\n",
    "        data  = np.concatenate([\n",
    "            open / close[-1], \n",
    "            high / close[-1],\n",
    "            low / close[-1],\n",
    "            close / close[-1], \n",
    "            volume / volume[-1]\n",
    "        ])\n",
    "        input_data_list.append(data)\n",
    "        #true_data_list.append(train_df[\"log_growing_rate\"][i])\n",
    "        true_data_list.append((train_df[\"growing_rate\"][i] - 1.0) * 100)\n",
    "\n",
    "    return np.array(input_data_list, dtype=np.float32), np.array(true_data_list, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_true = get_data_list(train_df)\n",
    "valid_input, valid_true = get_data_list(valid_df)\n",
    "\n",
    "train_data_dir.mkdir(exist_ok=True)\n",
    "np.savez(output_file_path, train_input, train_true, valid_input, valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load(output_file_path)\n",
    "train_input, train_true, valid_input, valid_true = npz[\"arr_0\"], npz[\"arr_1\"], npz[\"arr_2\"], npz[\"arr_3\"]\n",
    "# train_true = (train_true > 20).astype(np.float32)[..., None]\n",
    "# valid_true = (valid_true > 20).astype(np.float32)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    #return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(y_true, y_pred)\n",
    "\n",
    "\n",
    "class Model(tf.keras.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def grad(model: tf.keras.Layer, x: tf.Tensor, y: tf.Tensor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(y, pred)\n",
    "    return pred, loss, tape.gradient(loss, model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "steps_per_epoch = 50000 // batch_size\n",
    "pos_ratio = 0.3\n",
    "pos_num = int(batch_size * pos_ratio)\n",
    "neg_num = batch_size - pos_num\n",
    "\n",
    "true_arr = np.concatenate([np.ones((pos_num, 1), dtype=np.float32), np.zeros((neg_num, 1), dtype=np.float32)])\n",
    "\n",
    "pos_train_input = train_input[(train_true > 20).reshape(-1)]\n",
    "neg_train_input = train_input[(train_true < 10).reshape(-1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step in range(steps_per_epoch):\n",
    "        pos_input = pos_train_input[np.random.randint(len(pos_train_input), size=pos_num), :]\n",
    "        neg_input = neg_train_input[np.random.randint(len(neg_train_input), size=neg_num), :]\n",
    "        input = np.concatenate([pos_input, neg_input])\n",
    "\n",
    "        pred, loss, grads = grad(model, input, true_arr)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(\"epoch = {}, step = {:2d}, loss = {}\".format(epoch, step, loss), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = model(valid_input)\n",
    "valid_pred_y = valid_pred > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = valid_pred_y[valid_true > 20].numpy().sum() / (valid_true > 20).sum()\n",
    "specificity = 1.0 - valid_pred_y[valid_true < 10].numpy().sum() / (valid_true < 10).sum()\n",
    "\n",
    "num_tp = valid_pred_y[valid_true > 20].numpy().sum()\n",
    "num_fp = valid_pred_y[valid_true < 10].numpy().sum()\n",
    "precision = num_tp / (num_tp + num_fp)\n",
    "\n",
    "print(\"recall = {:.3f}, precision = {:.3f}, specificity = {:.3f}\".format(recall, precision, specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tp, num_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
