{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List, Set\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "import tensorflow as tf\n",
    "\n",
    "import stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"~/work/Projects/stock/data/sp500_companies.csv\")\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GICS Sector\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = df[\"Symbol\"].tolist()\n",
    "symbol = symbols[0]\n",
    "data_df = pd.read_csv(stock.DATA_DIR / \"sp500\" / f\"{symbol}.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = df[\"Symbol\"].to_list()\n",
    "\n",
    "class DatasetParams(BaseModel):\n",
    "    # Path to the csv file which contains list of symbols to use.\n",
    "    symbols_csv_path: Path\n",
    "    # Path to the directory where the stock data is stored.\n",
    "    # Assuming the symbol's data is storead to {data_dir}/{symbol}.csv.\n",
    "    data_dir: Path  \n",
    "    # Path to the directory where the dataset will be stored.\n",
    "    dataset_path: Path\n",
    "    # window parameters\n",
    "    input_width: int = 30\n",
    "    output_width: int = 30\n",
    "    stride: int = 1\n",
    "    shift: int = 1\n",
    "    #\n",
    "    batch_size: int = 32\n",
    "    prefetch: int = 32\n",
    "\n",
    "class Dataset:\n",
    "    STOCK_DATA_KEYS = [\"start\", \"high\", \"low\", \"end\", \"volume\"]\n",
    "    TRAIN_VAL_TEST_RATIO = [0.7, 0.1, 0.2]\n",
    "\n",
    "    def __init__(self, params: DatasetParams):\n",
    "        self.params = params\n",
    "        df = pd.read_csv(self.params.symbols_csv_path)\n",
    "        self.symbols = df[\"Symbol\"].to_list()\n",
    "        self.data = self.load_data()\n",
    "\n",
    "        self.input_labels = np.arange(self.params.input_width)\n",
    "        self.ouptut_labels = self.input_labels + self.params.shift\n",
    "\n",
    "        if not self.params.dataset_path.exists():        \n",
    "            self.save_data(self.paramns.dataset_path)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"`self.symbols`に格納されている銘柄の株価データを読み込む。\n",
    "        timestampがindexになるように整列したnumpy array (2d)を返す\n",
    "        (行 : timestamp, 列 : 各銘柄の株価(start, hihg, low, end, volume))\n",
    "        \"\"\"\n",
    "        if self.params.dataset_path.exists():\n",
    "            return np.load(self.params.dataset_path)\n",
    "\n",
    "        dfs: List[pd.DataFrame] = []\n",
    "        timestamps: Set[np.int64] = set()\n",
    "        for symbol in symbols:\n",
    "            data_csv = self.params.data_dir / f\"{symbol}.csv\"\n",
    "            if not data_csv.exists():\n",
    "                continue\n",
    "            df = pd.read_csv(data_csv)\n",
    "            timestamps.update(df.timestamp.to_list())\n",
    "            dfs.append(df)\n",
    "\n",
    "        arr = np.zeros((len(timestamps), len(dfs) * len(self.STOCK_DATA_KEYS) + 1))\n",
    "        timestamps = sorted(list(timestamps))\n",
    "        arr[:, 0] = timestamps\n",
    "\n",
    "        stock.logger.debug(\"Start create dataset array : \")\n",
    "        for i, df in tqdm(enumerate(dfs)):\n",
    "            for j, ts in enumerate(timestamps):\n",
    "                if ts in df.timestamp.to_list():\n",
    "                    idx = df[df.timestamp == ts].index[0]\n",
    "                    arr[idx, i * len(self.STOCK_DATA_KEYS) + 1: (i + 1) * len(self.STOCK_DATA_KEYS) + 1] = df.loc[idx, self.STOCK_DATA_KEYS].to_numpy()\n",
    "        stock.logger.debug(\"Finish create dataset array\")\n",
    "        return arr\n",
    "\n",
    "    def save_data(self, path: Path):\n",
    "        \"\"\"`self.data`をnpyファイルに保存する\"\"\"\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(path, self.data)\n",
    "\n",
    "    def make_dataset(self, is_train):\n",
    "        def map_func(window: tf.Tensor):\n",
    "            arr = list(window.as_numpy_iterator())\n",
    "            input = arr[:self.params.input_width]\n",
    "            output = arr[self.params.shift:]\n",
    "            return input, output\n",
    "\n",
    "        window_size = self.params.shift + self.params.output_width\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.data.astype(np.float32))\n",
    "        ds = ds.window(window_size, stride=self.params.stride, shift=1, drop_remainder=True)\n",
    "        ds = ds.map(lambda x: tf.py_function(func=map_func, inp=[x], Tout=[\n",
    "            tf.float32, tf.float32\n",
    "        ]), num_parallel_calls=8)\n",
    "        if is_train:\n",
    "            ds = ds.shuffle(len(self.data), reshuffle_each_iteration=True)\n",
    "        ds = ds.batch(self.params.batch_size, drop_remainder=is_train).prefetch(self.params.prefetch)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = DatasetParams(\n",
    "    symbols_csv_path=csv_path,\n",
    "    dataset_path = stock.DATA_DIR / \"dataset\" / \"sp500_20221203.npy\",\n",
    "    data_dir=stock.DATA_DIR / \"sp500\",\n",
    ")\n",
    "dataset = Dataset(params)\n",
    "ds = dataset.make_dataset(is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ds.take(10):\n",
    "    print(list(x[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0925d657e30d00cf1a4f19c1b7a91cf3b3559e9055505eaa2fd1b9771cefd6bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
