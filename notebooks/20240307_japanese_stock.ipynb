{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tomorrow_values(html, target_date: int):\n",
    "    soup = BeautifulSoup(html)\n",
    "    tables = soup.find_all(\"div\", {\"id\": \"stock_kabuka_table\"})[0]\n",
    "    table = tables.find_all(\"table\", {\"class\": \"stock_kabuka_dwm\"})[0]\n",
    "\n",
    "    for row in table.find(\"tbody\").find_all(\"tr\")[::-1]:\n",
    "        date = int(\"20{}{}{}\".format(*row.find(\"th\").text.split(\"/\")))\n",
    "        if date <= target_date:\n",
    "            continue\n",
    "        cols = row.find_all(\"td\")\n",
    "        values = [int(float(val_str.text.replace(\",\", \"\"))) for val_str in cols[:4]]\n",
    "        return date, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nikkei_url = \"https://kabutan.jp/stock/kabuka?code=0000&ashi=day&page={}\"\n",
    "topix_url = \"https://kabutan.jp/stock/kabuka?code=0010&ashi=day&page={}\"\n",
    "nasdaq_url = \"https://us.kabutan.jp/indexes/%5EIXIC/historical_prices/daily?page={}\"\n",
    "\n",
    "def search_stock_table_rows(soup):\n",
    "    res = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        thead = table.find(\"tr\")\n",
    "        headers = [th.text for th in thead.find_all(\"th\")]\n",
    "        if len(headers) >= 4 and headers[1] == \"始値\" and headers[2] == \"高値\" and headers[3] == \"安値\" and headers[4] == \"終値\":\n",
    "            res += table.find_all(\"tr\")[1:]\n",
    "    return res\n",
    "\n",
    "def get_values(base_url, max_page=1):\n",
    "    results = {}\n",
    "    for i in range(max_page):\n",
    "        url = base_url.format(i + 1)\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        rows = search_stock_table_rows(soup)\n",
    "\n",
    "        if len(rows) == 0:\n",
    "            print(f\"stock table not found : {url}\")\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = [col.text for col in row.findChildren(recursive=False)]\n",
    "            date = \"20{}{}{}\".format(*cols[0].split(\"/\"))\n",
    "            values = [float(val_str.replace(\",\", \"\")) for val_str in cols[1:5]]\n",
    "            results[date] = values\n",
    "        time.sleep(0.2)\n",
    "    return results\n",
    "\n",
    "nikkei_values = get_values(nikkei_url, 10)\n",
    "topix_values = get_values(topix_url, 10)\n",
    "nasdaq_values = get_values(nasdaq_url, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://us.kabutan.jp/stocks/NVDA/historical_prices/daily?page={}\"\n",
    "nvidia_values = get_values(base_url, 10)\n",
    "nomura_values = get_values(\"https://kabutan.jp/stock/kabuka?code=6254&ashi=day&page={}\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(r\"../data/sandbox/202402_downward.csv\")\n",
    "url_template = \"https://kabutan.jp/stock/kabuka?code={}\"\n",
    "\n",
    "with open(csv_path, \"r\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    data = list(csv_reader)[1:]\n",
    "\n",
    "results = []\n",
    "for row in data:\n",
    "    date, code = row\n",
    "    target_url = url_template.format(code)\n",
    "    res = requests.get(target_url)\n",
    "    date, values = get_tomorrow_values(res.text, int(date))\n",
    "    results.append([date, code, values])\n",
    "    print(code)\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_up = 0\n",
    "sum_up = 0\n",
    "cnt_down = 0\n",
    "sum_down = 0\n",
    "for res in results:\n",
    "    up_rate = (res[2][3] - res[2][0]) / res[2][0]\n",
    "    if up_rate > 0:\n",
    "        cnt_up += 1\n",
    "        sum_up += up_rate\n",
    "    elif up_rate < 0:\n",
    "        cnt_down += 1\n",
    "        sum_down += up_rate\n",
    "\n",
    "print(\"count up = {}, rate = {}, down = {}, rate = {}\".format(cnt_up, sum_up / cnt_up, cnt_down, sum_down / cnt_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_up / len(results), cnt_down / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_per_day = {}\n",
    "down_per_day = {}\n",
    "\n",
    "for res in results:\n",
    "    day = res[0]\n",
    "    if day not in up_per_day:\n",
    "        up_per_day[day] = []\n",
    "        down_per_day[day] = []\n",
    "    up_rate = (res[2][3] - res[2][0]) / res[2][0]\n",
    "    if up_rate > 0:\n",
    "        up_per_day[day].append(up_rate)\n",
    "    elif up_rate < 0:\n",
    "        down_per_day[day].append(up_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(up_per_day.keys()):\n",
    "    num_up = len(up_per_day[key])\n",
    "    mean_up = sum(up_per_day[key]) / num_up if num_up > 0 else 0\n",
    "    num_down = len(down_per_day[key])\n",
    "    mean_down = sum(down_per_day[key]) / num_down if num_down > 0 else 0\n",
    "    nikkei = nikkei_values[str(key)]\n",
    "    nikkei_up = (nikkei[3] - nikkei[0]) / nikkei[0]\n",
    "    topix = topix_values[str(key)]\n",
    "    topix_up = (topix[3] - topix[0]) / topix[0]\n",
    "    print(\"key : {}, up: num {:2d}, mean {:.3f}, down : num {:2d}, mean {:.3f}, nikkei : {:.3f}, topix : {:.3f}\".format(\n",
    "        key, num_up, mean_up, num_down, mean_down, nikkei_up, topix_up\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_up_rates_pairs(us_values, jp_values):\n",
    "    jp_idx = 0\n",
    "    us_idx = 0\n",
    "\n",
    "    jp_key_ints = sorted([int(val) for val in jp_values.keys()])\n",
    "    us_key_ints = sorted([int(val) for val in us_values.keys()])\n",
    "\n",
    "    up_rates = []\n",
    "    while jp_idx < len(jp_values) and us_idx < len(us_values):\n",
    "        jp_cur = jp_key_ints[jp_idx]\n",
    "        us_keys = []\n",
    "        while us_key_ints[us_idx] < jp_cur:\n",
    "            us_keys.append(us_key_ints[us_idx])\n",
    "            us_idx += 1\n",
    "            if us_key_ints[us_idx] >= jp_cur:\n",
    "                break\n",
    "\n",
    "        if jp_idx > 0 and len(us_keys) > 0:\n",
    "            us_previous_end = us_values[str(us_key_ints[us_idx - len(us_keys) - 1])][3]\n",
    "            us_start = us_values[str(us_keys[0])][0]\n",
    "            us_end = us_values[str(us_keys[-1])][3]\n",
    "            us_up_rate = (us_end - us_start) / us_end\n",
    "            us_end_up_rate = (us_end - us_previous_end) / us_previous_end\n",
    "\n",
    "            jp_start = jp_values[str(jp_cur)][0]\n",
    "            jp_end = jp_values[str(jp_cur)][3]\n",
    "            jp_up_rate = (jp_end - jp_start) / jp_start\n",
    "\n",
    "            #print(jp_cur, us_keys, us_up_rate, jp_up_rate, us_end_up_rate)\n",
    "            up_rates.append([jp_up_rate, us_up_rate, us_end_up_rate, jp_cur])\n",
    "\n",
    "        jp_idx += 1\n",
    "\n",
    "    up_rates = np.array(up_rates)\n",
    "    return up_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_rates = get_up_rates_pairs(nvidia_values, nomura_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_str = \"\"\"■串カツ田中ホールディングス <3547> [東証Ｓ]\n",
    "2月既存店売上高は前年同月比0.6％減と前年割れに転じた。\n",
    "\n",
    "■イルグルム <3690> [東証Ｇ]\n",
    "2月売上高は前年同月比7.4％減と前年割れに転じた。\n",
    "\n",
    "■ヘッドウォータース <4011> [東証Ｇ]\n",
    "東証と日証金が8日売買分から信用取引に関する臨時措置を実施する。\n",
    "\n",
    "■アサンテ <6073> [東証Ｐ]\n",
    "2月売上高は前年同月比4.4％減と2ヵ月ぶりに前年割れとなった。\n",
    "\n",
    "■ナ・デックス <7435> [東証Ｓ]\n",
    "今期経常を21％下方修正。\n",
    "\n",
    "■アゴーラ　ホスピタリティー　グループ <9704> [東証Ｓ]\n",
    "東証と日証金が8日売買分から信用取引に関する臨時措置を実施する。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "regex = re.compile(\"■.*<(\\d*)>\")\n",
    "for line in raw_str.split(\"\\n\"):\n",
    "    res = regex.search(line)\n",
    "    if res is not None:\n",
    "        print(res.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
