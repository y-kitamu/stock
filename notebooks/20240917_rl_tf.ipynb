{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflowでrlを実装するためのsandbox\n",
    "# かんたんなアルゴから実装していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import  gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, action_space: gym.Space, observation_space: gym.Space):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "\n",
    "    def policy(self, observation):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def learn(self, action, observation, next_observation, reward):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    class Params(BaseModel):\n",
    "        max_steps: int = 1000\n",
    "\n",
    "    def __init__(self, params: Params, env: gym.Env, agent: Agent):\n",
    "        self.params = params\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.history = []\n",
    "\n",
    "    def train(self):\n",
    "        self.history = []\n",
    "        observation, info = env.reset()\n",
    "        history = [(observation, info, 0, None)]\n",
    "\n",
    "        good_action = []\n",
    "        good_observation = []\n",
    "        good_next_observation = []\n",
    "        good_reward = []\n",
    "\n",
    "        bad_action = []\n",
    "        bad_observation = []\n",
    "        bad_next_observation = []\n",
    "        bad_reward = []\n",
    "\n",
    "        for epoch in range(self.params.max_steps):\n",
    "            batch_action = []\n",
    "            batch_observation = []\n",
    "            batch_next_observation = []\n",
    "            batch_reward = []\n",
    "\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "                action = self.agent.policy(observation)\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                batch_action.append(action)\n",
    "                batch_observation.append(observation)\n",
    "                batch_next_observation.append(next_observation)\n",
    "                batch_reward.append(reward)\n",
    "\n",
    "                observation = next_observation\n",
    "                history.append((observation, info, reward, action))\n",
    "                \n",
    "                if len(good_action) + len(bad_action) > 100:\n",
    "                    \n",
    "                    train_action = []\n",
    "                    train_observation = []\n",
    "                    train_next_observation = []\n",
    "                    train_reward = []\n",
    "\n",
    "                    if len(good_action) > 0:\n",
    "                        good_index = np.random.randint(0, len(good_action), 2000)\n",
    "                        train_action.extend([good_action[i] for i in good_index])\n",
    "                        train_observation.extend([good_observation[i] for i in good_index])\n",
    "                        train_next_observation.extend([good_next_observation[i] for i in good_index])\n",
    "                        train_reward.extend([good_reward[i] for i in good_index])\n",
    "                    if len(bad_action) > 0:\n",
    "                        bad_index = np.random.randint(0, len(bad_action), 2000)\n",
    "                        train_action.extend([bad_action[i] for i in bad_index])\n",
    "                        train_observation.extend([bad_observation[i] for i in bad_index])\n",
    "                        train_next_observation.extend([bad_next_observation[i] for i in bad_index])\n",
    "                        train_reward.extend([bad_reward[i] for i in bad_index])\n",
    "                    \n",
    "                    self.agent.learn(train_action, train_observation, train_next_observation, train_reward)\n",
    "                        \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            self.history.append(history)\n",
    "            observation, info = env.reset()\n",
    "            history = [(observation, info, 0, None)]\n",
    "            self.render(history)\n",
    "            print(\"epoch : {}, total reward : {}\".format(epoch, total_reward))\n",
    "\n",
    "            if total_reward > 100:\n",
    "                good_action.extend(batch_action)\n",
    "                good_observation.extend(batch_observation)\n",
    "                good_next_observation.extend(batch_next_observation)\n",
    "                good_reward.extend(batch_reward)\n",
    "            else:\n",
    "                bad_action.extend(batch_action)\n",
    "                bad_observation.extend(batch_observation)\n",
    "                bad_next_observation.extend(batch_next_observation)\n",
    "                bad_reward.extend(batch_reward)\n",
    "\n",
    "        if len(history) > 1:\n",
    "            self.history.append(history)\n",
    "            \n",
    "        env.close()\n",
    "\n",
    "    def render(self, history):\n",
    "        plt.plot([h[2] for h in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent\n",
    "class RandomAgent(Agent):\n",
    "    def policy(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def learn(self, action, observation, next_observation, reward):\n",
    "        pass\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "trainer = Trainer(Trainer.Params(max_steps=1000), env, RandomAgent(env.action_space, env.observation_space))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCModel(tf.keras.Model):\n",
    "    def __init__(self, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# deep q network\n",
    "class DQNAgent(Agent):\n",
    "    def __init__(self, action_space: gym.Space, observation_space: gym.Space):\n",
    "        super().__init__(action_space, observation_space)\n",
    "        self.model = FCModel(action_space.n)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "        self.epsilon = 0.2\n",
    "        self.count = 0\n",
    "        self.factor = 1000000\n",
    "\n",
    "    def policy(self, observation, return_softmax=False, training=True):\n",
    "        observation = np.array(observation)\n",
    "        if len(observation.shape) == 1:\n",
    "            observation = observation[None]\n",
    "        x = tf.convert_to_tensor(observation, dtype=tf.float32)\n",
    "        x = self.model(x)\n",
    "        if return_softmax:\n",
    "            return x\n",
    "        if training:\n",
    "            # epsilon greedy\n",
    "            if np.random.rand() > self.epsilon - self.count / self.factor:\n",
    "                return tf.argmax(x, axis=-1).numpy()[0]\n",
    "            else:\n",
    "                return self.action_space.sample()\n",
    "        else:\n",
    "            return tf.argmax(x, axis=-1).numpy()[0]\n",
    "    \n",
    "\n",
    "    def learn(self, action, observation, next_observation, reward):\n",
    "        oh_action = tf.one_hot(action, self.action_space.n)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q = tf.reduce_sum(self.policy(observation, return_softmax=True) * oh_action, axis=-1)\n",
    "            q_next = tf.reduce_max(self.policy(next_observation, return_softmax=True), axis=-1)\n",
    "            loss = tf.reduce_mean(tf.square(q - (reward + q_next)))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        #self.count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.action_space, env.observation_space)\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"ansi\")\n",
    "trainer = Trainer(Trainer.Params(max_steps=1000), env, agent)\n",
    "try:\n",
    "    trainer.train()\n",
    "except:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sum([d[2] for d in h]) for h in trainer.history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([d[2] for h in trainer.history  for d in h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.history[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[d[2] for d in record] for record in trainer.history]\n",
    "plt.plot(data[-2])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
